---
title: "Continual Learning with Graph Reservoirs: Preliminary experiments in graph classification"
collection: publications
category: conferences
permalink: /publication/2024-10-09-continual-learning-with-graph-reservoirs
excerpt: "GESN relieves part of catastrophic forgetting in the continual learning setting by avoiding training representations for graph classification."
date: 2024-10-09
venue: 'ESANN'
paperurl: 'https://doi.org/10.14428/esann/2024.ES2024-21'
citation: 'D. Tortorella, A. Micheli (2024). &quot;Continual Learning with Graph Reservoirs: Preliminary experiments in graph classification.&quot; <i>Proceedings of the 32nd European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN 2024)</i>, pp. 35-40.'
---

![Graphical abstract](/images/2024-10-09-continual-learning-with-graph-reservoirs.png)

Continual learning aims to address the challenge of catastrophic forgetting in training models where data patterns are non-stationary. Previous research has shown that fully-trained graph learning models are particularly affected by this issue. One approach to lifting part of the burden is to leverage the representations provided by a training-free reservoir computing model. In this work, we evaluate for the first time different continual learning strategies in conjunction with Graph Echo State Networks, which have already demonstrated their efficacy and efficiency in graph classification tasks.