---
title: "Investigating Time-Scales in Deep Echo State Networks for Natural Language Processing"
collection: publications
category: conferences
permalink: /publication/2025-09-12-investigating-time-scales-in-deep-esn
excerpt: 'An analysis of time-scales in Deep Bidirectional ESNs applied to sentence-level and token-level NLP tasks.'
date: 2025-09-12
venue: 'ICANN'
paperurl: 'https://doi.org/10.1007/978-3-032-04552-2_18'
citation: 'C. Baccheschi, A. Bondielli, A. Lenci, A. Micheli, L. Passaro, M. Podda, D. Tortorella (2025). &quot;Investigating Time-Scales in Deep Echo State Networks for Natural Language Processing.&quot; <i>Artificial Neural Networks and Machine Learning. ICANN 2025 International Workshops and Special Sessions</i>, LNCS vol. 16072, pp. 188-200.'
---

![Graphical abstract](/images/2025-09-12-investigating-time-scales-in-deep-esn.png)

Reservoir Computing (RC) enables efficiently-trained deep Recurrent Neural Networks (RNNs) by removing the need to train the hierarchy of representations of the input sequences. In this paper, we analyze the performance and the dynamical behavior of RC models, specifically Deep Bidirectional Echo State Networks (Deep-BiESNs), applied to Natural Language Processing (NLP) tasks. We compare the performance of Deep-BiESNs against fully-trained NLP baseline models on six common NLP tasks: three sequence-to-vector tasks for sequence-level classification and three sequence-to-sequence tasks for token-level labeling. Experimental results demonstrate that Deep-BiESNs achieve comparable or superior performance to these baseline models. We then adapt the class activation mapping technique for explainability to analyze the dynamical properties of these deep RC models, highlighting how the hierarchy of representations in Deep-BiESNs layers contributes to forming the class prediction in the different NLP tasks. Investigating time scales in deep RNN layers is highly relevant for NLP because language inherently involves dependencies that occur over various temporal horizons. The findings not only underscore the potential of Deep ESNs as a competitive and efficient alternative for NLP applications, but also contribute to a deeper understanding of how to effectively model such architectures for addressing other NLP challenges.